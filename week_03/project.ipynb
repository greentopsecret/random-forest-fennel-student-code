{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODOs**  \n",
    "- [x] Challenge description\n",
    "- [ ] Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Challenge description](#Challenge-description)\n",
    "* [Data exploration](#Data-exploration)\n",
    "\n",
    "#TODO: finish or delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this weekâ€™s project is to build and train a regression model on the Capital Bike Share (Washington, D.C.) Kaggle data set, in order to predict demand for bicycle rentals at any given hour, based on time and weather, e.g.\n",
    "\n",
    "**Data description**  \n",
    "[https://www.kaggle.com/c/bike-sharing-demand](https://www.kaggle.com/c/bike-sharing-demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/train.csv', parse_dates=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X = df[df.columns.difference(['count', 'registered', 'casual'])].copy(deep=True)\n",
    "y = df['count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_train.sort_index().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.sort_index().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tmp = df[['season', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count']]\n",
    "sns.heatmap(\n",
    "    tmp.corr(), \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "PAIRPLOT_FROM = '2012-01-01'\n",
    "PAIRPLOT_TO = '2013-01-01'\n",
    "filename = \"./output/pairplot-\" + PAIRPLOT_FROM + \" --\" + PAIRPLOT_TO + \".png\"\n",
    "\n",
    "# Drawing a pairplot takes several minutes, that's why better to save (cache) image once it's created for the first time\n",
    "file = Path(filename)\n",
    "if not file.is_file():\n",
    "    tmp = df.loc[PAIRPLOT_FROM:PAIRPLOT_TO, ['atemp', 'temp', 'humidity', 'season', 'weather', 'windspeed', 'count']]\n",
    "    tmp = extract_datetime_data(tmp) # TODO: move this function to the top\n",
    "    plot = sns.pairplot(tmp, hue='count')\n",
    "    plot.savefig(file)\n",
    "else:\n",
    "    display(HTML('<img src=\"'+filename+'\"></img>'))\n",
    "    # display.Image(filename) # not working :(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: on a heatmap above we can clearly see two hightly correlated variables - temp and atemp. We will use only one of them (temp) in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ask about \"weather\" and \"season\" variables - does it make sense to have them in the heatmap?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average count by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.to_datetime(df['datetime']).dt.hour)['count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average count by day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.to_datetime(df['datetime']).dt.weekday)['count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total count by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.to_datetime(df['datetime']).dt.month)['count'].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of bike rents during a week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df[(df['datetime'] > '2012-12-12') & (df['datetime'] <= '2013-12-19')], x=\"datetime\", y=\"count\", title='Count of bike rents during a week')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create hourly weights dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby(pd.to_datetime(df['datetime']).dt.hour)['count'].sum().sort_values().reset_index().drop(columns='count').to_dict()\n",
    "hour_weight_dict = {v:k for k, v in tmp['datetime'].items()}\n",
    "# hour_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create monthly weights dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby(pd.to_datetime(df['datetime']).dt.month)['count'].sum().sort_values().reset_index().drop(columns='count').to_dict()\n",
    "month_weight_dict = {v:k+1 for k, v in tmp['datetime'].items()}\n",
    "month_weight_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.pipeline as pipeline\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "# datetime_pipeline = pipeline.make_pipeline(\n",
    "#     FunctionTransformer(extract_datetime_data)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     # ('extract_datetime_data', datetime_pipeline, ['season']),\n",
    "#     ('min_max_scaler', MinMaxScaler(), ['temp', 'humidity', 'windspeed']),\n",
    "#     ('do_nothing', 'passthrough', ['season', 'weather'])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for extracting date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(df)\n",
    "    # df['hour'] = df.index.hour\n",
    "    df['hour_weight'] = pd.to_datetime(df['datetime']).dt.hour.map(hour_weight_dict)\n",
    "    # df['weekday'] = df.index.weekday\n",
    "    # df['month'] = df.index.month\n",
    "    df['month_weight'] = pd.to_datetime(df['datetime']).dt.month.map(month_weight_dict)\n",
    "\n",
    "    df.drop(columns=\"datetime\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# TODO: add assertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor.fit(X_train)\n",
    "# X_train_fe = preprocessor.transform(X_train)\n",
    "# X_test_fe = preprocessor.transform(X_test)\n",
    "col = ['season', 'weather', 'temp', 'humidity', 'windspeed', 'datetime']\n",
    "\n",
    "# col = ['temp']\n",
    "X_train_fe = X_train[col].copy(deep=True)\n",
    "X_test_fe = X_test[col].copy(deep=True)\n",
    "\n",
    "X_train_fe = extract_datetime_data(X_train_fe)\n",
    "X_test_fe = extract_datetime_data(X_test_fe)\n",
    "\n",
    "assert pd.DataFrame(X_train_fe).isna().sum().unique().size == 1\n",
    "assert pd.DataFrame(X_train_fe).isna().sum()[0] == 0\n",
    "\n",
    "X_train_fe.sort_index().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def apply_min_max_scaller(df):\n",
    "\n",
    "    transformer = ColumnTransformer([\n",
    "        ('mmscaler', MinMaxScaler(), ['temp', 'windspeed', 'humidity', 'season', 'weather', 'hour_weight', 'month_weight'])\n",
    "    ], remainder='drop')\n",
    "\n",
    "    return pd.DataFrame(transformer.fit_transform(df), columns=transformer.get_feature_names_out())\n",
    "\n",
    "X_train_fe2 = apply_min_max_scaller(X_train_fe)\n",
    "X_test_fe2 = apply_min_max_scaller(X_test_fe)\n",
    "\n",
    "assert pd.DataFrame(X_train_fe2).isna().sum().unique().size == 1\n",
    "assert pd.DataFrame(X_test_fe2).isna().sum()[0] == 0\n",
    "\n",
    "X_train_fe2.sort_index().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pt = PolynomialFeatures(interaction_only=True)\n",
    "X_train_fe3 = pd.DataFrame(pt.fit_transform(X_train_fe2), columns=pt.get_feature_names_out())\n",
    "X_test_fe3 = pd.DataFrame(pt.fit_transform(X_test_fe2), columns=pt.get_feature_names_out())\n",
    "\n",
    "assert pd.DataFrame(X_train_fe3).isna().sum().unique().size == 1\n",
    "assert pd.DataFrame(X_test_fe3).isna().sum()[0] == 0\n",
    "\n",
    "X_train_fe3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pd.DataFrame(X_train_fe3).isna().sum().unique().size == 1\n",
    "assert pd.DataFrame(X_train_fe3).isna().sum()[0] == 0\n",
    "\n",
    "# TODO: move it or get rid of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression().fit(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test_fe)\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "import numpy as np\n",
    "\n",
    "def rmsle(p,a):\n",
    "    return np.sqrt(mean_squared_log_error(p,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSLE:\", rmsle(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Coefficients:\", lr_model.coef_)\n",
    "# print(\"Intercept   :\", lr_model.intercept_)\n",
    "\n",
    "# print(\"train score :\", lr_model.score(X_train_fe, y_train))\n",
    "# print(\"test score  :\", lr_model.score(X_test_fe, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "pr_model = PoissonRegressor(alpha=1)\n",
    "pr_model.fit(X_train_fe3, y_train)\n",
    "y_pred = pr_model.predict(X_test_fe3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate RMSLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "\n",
    "def rmslr(y_true, y_pred, **kwargs):\n",
    "   return mean_squared_log_error(y_true, y_pred, **kwargs)**0.5\n",
    "\n",
    "rmslr_scorer = make_scorer(rmslr, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmslr(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.05, 0.1, 0.2, 0.5, 0.75, 1], \n",
    "    'fit_intercept': [True, False],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "g = GridSearchCV(pr_model, param_grid, cv=5, scoring=rmslr_scorer, return_train_score=True )\n",
    "g.fit(X_train_fe3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(g.cv_results_)\n",
    "res.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['mean_test_score', 'mean_train_score', 'mean_fit_time',\n",
    "            'param_alpha', 'param_fit_intercept', 'param_max_iter']\n",
    "\n",
    "res.sort_values('mean_test_score', ascending=False)[col_names].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_model_best = g.best_estimator_\n",
    "\n",
    "y_pred_train =pr_model_best.predict(X_train_fe3)\n",
    "y_pred = pr_model_best.predict(X_test_fe3)\n",
    "\n",
    "rmslr(y_test, y_pred), rmslr(y_train, y_pred_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
