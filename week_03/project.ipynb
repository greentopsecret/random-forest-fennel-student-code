{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "- [ ] Simplify data transformation: use ColumnTransformer and pipeline  \n",
    "- [ ] Implement dictionary mapping for temperature (same as for month and time)  \n",
    "- [ ] Implement dictionary mapping for multiplied variables (temperature x hour)  \n",
    "- [ ] Add trend for count of bikes (it grows over time)  \n",
    "- [ ] GridSearchCV: firstly filter models with a small diff between train and test scores and take the best model out of them  \n",
    "- [ ] Scatterplot with known and predicted points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Challenge description](#Challenge-description)\n",
    "- [Data exploration](#Data-exploration)\n",
    "    - [Load data](#Load-data)\n",
    "    - [Train/Test-split](#Train/Test-split)\n",
    "    - [Heatmap](#Heatmap)\n",
    "    - [Pairplot](#Pairplot)\n",
    "    - [Check-NaN-values](#Check-NaN-values)\n",
    "    - [Average-count-by-hour](#Average-count-by-hour)\n",
    "    - [Average-count-by-day-of-the-week](#Average-count-by-day-of-the-week)\n",
    "    - [Total-count-by-month](#Total-count-by-month)\n",
    "    - [Count-of-bike-rents-during-a-week](#Count-of-bike-rents-during-a-week)\n",
    "- [Feature-engineering](#Feature-engineering)\n",
    "    - [Extract-date-time-data](#Extract-date-time-data)\n",
    "    - [Find-and-remove-features-with-hight-p-value](#Find-and-remove-features-with-hight-p-value)\n",
    "    - [Apply-min/max-scaller](#Apply-min/max-scaller)\n",
    "    - [Apply-polinomial-features-preprocessing](#Apply-polinomial-features-preprocessing)\n",
    "- [Models](#Models)\n",
    "    - [Linear-Regression-model](#Linear-Regression-model)\n",
    "        - [Predict-results](#Predict-results)\n",
    "        - [Evaluate-model](#Evaluate-model)\n",
    "        - [Feature-importance](#Feature-importance)\n",
    "    - [Poisson-Regressor-model](#Poisson-Regressor-model)\n",
    "        - [Calculate-RMSLR](#Calculate-RMSLR)\n",
    "        - [Grid-Search-Cross-Validation](#Grid-Search-Cross-Validation)\n",
    "    - [Random-Forest-Regressor](#Random-Forest-Regressor)\n",
    "        - [Calculate-RMSLR](#Calculate-RMSLR)\n",
    "        - [Grid-Search-Cross-Validation](#Grid-Search-Cross-Validation)\n",
    "        - [Feature-importance](#Feature-importance)\n",
    "        - [Ensember-Regressor](#Ensember-Regressor)\n",
    "- [Kaggle-submission](#Kaggle-submission)\n",
    "    - [Submission-result](#Submission-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --no-input plotly\n",
    "# !pip install --no-input seaborn\n",
    "# !pip install --no-input numpy\n",
    "# !pip install --no-input pandas\n",
    "# !pip install --no-input sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this weekâ€™s project is to build and train a regression model on the Capital Bike Share (Washington, D.C.) Kaggle data set, in order to predict demand for bicycle rentals at any given hour, based on time and weather, e.g.\n",
    "\n",
    "**Data description**  \n",
    "[https://www.kaggle.com/c/bike-sharing-demand](https://www.kaggle.com/c/bike-sharing-demand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/train.csv', parse_dates=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = df[df.columns.difference(['count', 'registered', 'casual'])].copy(deep=True)\n",
    "y = df['count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tmp = df[['season', 'weather', 'temp', 'atemp', 'humidity', 'windspeed', 'count']]\n",
    "sns.heatmap(\n",
    "    tmp.corr(), \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "PAIRPLOT_FROM = '2012-01-01'\n",
    "PAIRPLOT_TO = '2013-01-01'\n",
    "filename = \"./output/pairplot-\" + PAIRPLOT_FROM + \" --\" + PAIRPLOT_TO + \".png\"\n",
    "\n",
    "# Drawing a pairplot takes several minutes, that's why better to save (cache) image once it's created for the first time\n",
    "file = Path(filename)\n",
    "if not file.is_file():\n",
    "    tmp = df.loc[PAIRPLOT_FROM:PAIRPLOT_TO, ['atemp', 'temp', 'humidity', 'season', 'weather', 'windspeed', 'count']]\n",
    "    tmp = extract_datetime_data(tmp) # TODO: move this function to the top\n",
    "    plot = sns.pairplot(tmp, hue='count')\n",
    "    plot.savefig(file)\n",
    "else:\n",
    "    display(HTML('<img src=\"'+filename+'\"></img>'))\n",
    "    # display.Image(filename) # not working :(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: on a heatmap above we can clearly see two hightly correlated variables - temp and atemp. We will use only one of them (temp) in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average count by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.to_datetime(df['datetime']).dt.hour)['count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average count by day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.to_datetime(df['datetime']).dt.weekday)['count'].mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total count by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(pd.to_datetime(df['datetime']).dt.month)['count'].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count by temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temp = df.groupby(by=\"temp\")['count'].mean()\n",
    "mean_temp.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "mean_temp = df.groupby(by=[\"temp\", \"weather\"])['count'].mean()\n",
    "fig = px.line(mean_temp.reset_index(), x='temp', y='count', color='weather')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temp = df.groupby(by=[\"temp\"])['count'].mean()\n",
    "fig = px.line(mean_temp.reset_index(), x='temp', y='count')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df.tail(7 * 24), x=\"datetime\", y=\"count\", title='Count of bike rents during a week')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fe = X_train.copy(deep=True).sort_index().reset_index().drop(columns=['index'])\n",
    "X_test_fe = X_test.copy(deep=True).sort_index().reset_index().drop(columns=['index'])\n",
    "y_train = y_train.sort_index().reset_index().drop(columns=['index'])\n",
    "y_test = y_test.sort_index().reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertEqualIndexes(df1: pd.DataFrame, df2: pd.DataFrame):\n",
    "    assert df1.sort_index().tail().index.equals(df2.sort_index().tail().index)\n",
    "    \n",
    "assertEqualIndexes(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract date time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monthly weights dictionary\n",
    "tmp = df.groupby(pd.to_datetime(df['datetime']).dt.month)['count'].sum().sort_values().reset_index().drop(columns='count').to_dict()\n",
    "month_weight_dict = {v:k+1 for k, v in tmp['datetime'].items()}\n",
    "\n",
    "# Create hourly weights dictionary\n",
    "tmp = df.groupby(pd.to_datetime(df['datetime']).dt.hour)['count'].sum().sort_values().reset_index().drop(columns='count').to_dict()\n",
    "hour_weight_dict = {v:k for k, v in tmp['datetime'].items()}\n",
    "\n",
    "# Function for extracting date features\n",
    "def extract_datetime_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(df)\n",
    "    df['hour_weight'] = pd.to_datetime(df['datetime']).dt.hour.map(hour_weight_dict)\n",
    "    df['month_weight'] = pd.to_datetime(df['datetime']).dt.month.map(month_weight_dict)\n",
    "\n",
    "    df.drop(columns=\"datetime\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# TODO: add assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fe = extract_datetime_data(X_train_fe)\n",
    "X_test_fe = extract_datetime_data(X_test_fe)\n",
    "\n",
    "# check that after transformation there are no NaN values\n",
    "def assertHasNoNa(df: pd.DataFrame):\n",
    "    assert pd.DataFrame(df).isna().sum().unique().size == 1\n",
    "    assert pd.DataFrame(df).isna().sum()[0] == 0\n",
    "\n",
    "\n",
    "assertHasNoNa(X_train_fe)\n",
    "assertHasNoNa(X_test_fe)\n",
    "assertEqualIndexes(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "def apply_bins_discretizer(df):\n",
    "\n",
    "    transformer = ColumnTransformer([\n",
    "        ('', KBinsDiscretizer(n_bins=4), ['temp'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    return pd.DataFrame(transformer.fit_transform(df), columns=transformer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# tmp = X_train_fe.copy(deep=True)\n",
    "# apply_bins_discretizer(tmp).value_counts()\n",
    "\n",
    "X_train_fe = apply_bins_discretizer(X_train_fe)\n",
    "X_test_fe = apply_bins_discretizer(X_test_fe)\n",
    "\n",
    "assertHasNoNa(X_train_fe)\n",
    "assertHasNoNa(X_test_fe)\n",
    "assertEqualIndexes(X_train_fe, y_train)\n",
    "\n",
    "X_train_fe.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply min/max scaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def apply_min_max_scaller(df):\n",
    "\n",
    "    transformer = ColumnTransformer([\n",
    "        ('', MinMaxScaler(), ['season', 'hour_weight', 'month_weight'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    return pd.DataFrame(transformer.fit_transform(df), columns=transformer.get_feature_names_out())\n",
    "\n",
    "X_train_fe = apply_min_max_scaller(X_train_fe)\n",
    "X_test_fe = apply_min_max_scaller(X_test_fe)\n",
    "\n",
    "assertHasNoNa(X_train_fe)\n",
    "assertHasNoNa(X_test_fe)\n",
    "assertEqualIndexes(X_train_fe, y_train)\n",
    "\n",
    "# X_train_fe.sort_index().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply polynomial features preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# pt2 = PolynomialFeatures(interaction_only=False)\n",
    "# X_train_fe_debug = pd.DataFrame(pt2.fit_transform(X_train_fe), columns=pt2.get_feature_names_out())\n",
    "# X_test_fe_debug = pd.DataFrame(pt2.fit_transform(X_test_fe), columns=pt2.get_feature_names_out())\n",
    "\n",
    "# X_train_fe_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pt = PolynomialFeatures(interaction_only=True)\n",
    "X_train_fe = pd.DataFrame(pt.fit_transform(X_train_fe), columns=pt.get_feature_names_out())\n",
    "X_test_fe = pd.DataFrame(pt.fit_transform(X_test_fe), columns=pt.get_feature_names_out())\n",
    "\n",
    "assertHasNoNa(X_train_fe)\n",
    "assertHasNoNa(X_test_fe)\n",
    "assertEqualIndexes(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and remove features with hight p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "from statsmodels.api import OLS \n",
    "\n",
    "mod = OLS(y_train.reset_index().drop(columns=['index']), X_train_fe.reset_index())\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # P-values of the next features are more than 0.05, so we can conclude they are statistically insignificant. \n",
    "# insignificant_features = [\n",
    "#     '__windspeed',\n",
    "#     '__temp __windspeed',\n",
    "#     '__temp __weather',\n",
    "#     '__windspeed __season',\n",
    "#     '__windspeed __month_weight',\n",
    "#     '__humidity __season',\n",
    "#     '__season __weather',\n",
    "#     '__season __month_weight',\n",
    "#     '__weather __month_weight', '__hour_weight __month_weight']\n",
    "\n",
    "# X_train_fe.drop(columns=insignificant_features, inplace=True)\n",
    "# X_test_fe.drop(columns=insignificant_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression().fit(X_train_fe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test_fe)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "y_pred_train = lr_model.predict(X_train_fe)\n",
    "y_pred_train[y_pred_train < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "import numpy as np\n",
    "\n",
    "def rmslr(p,a):\n",
    "    return np.sqrt(mean_squared_log_error(p,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'rmslr (train data): %.4f; rmslr (test data): %.4f' % (rmslr(y_train, y_pred_train), rmslr(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(np.abs(lr_model.coef_[0]), index=lr_model.feature_names_in_, columns=['coef']).sort_values(by='coef', ascending=False)\n",
    "\n",
    "px.bar(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: permutation feature importance - https://scikit-learn.org/stable/modules/permutation_importance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PoissonRegressor\n",
    "\n",
    "pr_model = PoissonRegressor(alpha=1)\n",
    "pr_model.fit(X_train_fe, y_train.values.ravel())\n",
    "y_pred = pr_model.predict(X_test_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate RMSLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "\n",
    "def rmslr(y_true, y_pred, **kwargs):\n",
    "    return mean_squared_log_error(y_true, y_pred, **kwargs)**0.5\n",
    "\n",
    "rmslr_scorer = make_scorer(rmslr, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'rmslr (train data): %.4f; rmslr (test data): %.4f' % (rmslr(y_train, y_pred_train), rmslr(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'alpha': [0.05, 0.1, 0.2, 0.5, 0.75, 1], \n",
    "    'fit_intercept': [True, False],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "g = GridSearchCV(pr_model, param_grid, cv=5, scoring=rmslr_scorer, return_train_score=True )\n",
    "g.fit(X_train_fe, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(g.cv_results_)\n",
    "res.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['mean_test_score', 'mean_train_score', 'mean_fit_time',\n",
    "            'param_alpha', 'param_fit_intercept', 'param_max_iter']\n",
    "\n",
    "res.sort_values('mean_test_score', ascending=False)[col_names].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_model_best = g.best_estimator_\n",
    "\n",
    "y_pred_train =pr_model_best.predict(X_train_fe)\n",
    "y_pred = pr_model_best.predict(X_test_fe)\n",
    "\n",
    "'rmslr (train data): %.4f; rmslr (test data): %.4f' % (rmslr(y_train, y_pred_train), rmslr(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train_fe, y_train.values.ravel())\n",
    "y_pred = rf_model.predict(X_test_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate RMSLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'rmslr (train data): %.4f; rmslr (test data): %.4f' % (rmslr(y_train, y_pred_train), rmslr(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting GridSearchCV with these parameters took 31m 34.1s\n",
    "# It output next best params: \n",
    "# {'ccp_alpha': 0.05,\n",
    "#  'max_depth': None,\n",
    "#  'max_features': 10,\n",
    "#  'max_leaf_nodes': None,\n",
    "#  'min_impurity_decrease': 0.0,\n",
    "#  'min_samples_leaf': 3,\n",
    "#  'min_samples_split': 2,\n",
    "#  'n_estimators': 100}\n",
    "# \n",
    "# param_grid = {\n",
    "#     'ccp_alpha': [0.0, 0.05], \n",
    "#     'n_estimators': [1, 100],\n",
    "#     'max_depth': [None, 5, 20],\n",
    "#     'min_samples_split': [2, 5],\n",
    "#     'min_samples_leaf': [1, 3],\n",
    "#     'max_features': ['sqrt', 'log2', 1, 10],\n",
    "#     'max_leaf_nodes': [None, 1, 10, 100],\n",
    "#     'min_impurity_decrease': [0.0, 0.2]\n",
    "# }\n",
    "# \n",
    "# g = GridSearchCV(rf_model, param_grid, cv=5, scoring=rmslr_scorer, return_train_score=True )\n",
    "# g.fit(X_train_fe3, y_train)\n",
    "# res = pd.DataFrame(g.cv_results_)\n",
    "# res.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_names = ['mean_test_score', 'mean_train_score', 'mean_fit_time', 'param_ccp_alpha', 'param_min_impurity_decrease']\n",
    "# res.sort_values('mean_test_score', ascending=False)[col_names].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model_best = g.best_estimator_\n",
    "rf_model_best = RandomForestRegressor(\n",
    "    ccp_alpha=0.05, \n",
    "    min_impurity_decrease=0.0, \n",
    "    min_samples_leaf=3, \n",
    "    min_samples_split=2, \n",
    "    n_estimators=100\n",
    ")\n",
    "rf_model_best.fit(X_train_fe, y_train.values.ravel())\n",
    "\n",
    "y_pred_train = rf_model_best.predict(X_train_fe)\n",
    "y_pred = rf_model_best.predict(X_test_fe)\n",
    "\n",
    "'rmslr (train data): %.4f; rmslr (test data): %.4f' % (rmslr(y_train, y_pred_train), rmslr(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO\n",
    "# corr = df.corr()\n",
    "# corr\n",
    "\n",
    "# upper_tri = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool))\n",
    "\n",
    "# to_drop = [column for column in upper_tri.columns if any(np.sqrt((upper_tri[column])**2) > 0.60)]\n",
    "# to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(x = rf_model.feature_names_in_, y=rf_model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(rf_model.feature_importances_, index=rf_model.feature_names_in_, columns=['coef'])\n",
    "tmp.sort_values(by='coef', ascending=False).head(5)['coef'].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: https://h1ros.github.io/posts/feature-importance/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensember Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "er = VotingRegressor([\n",
    "#     ('lr', lr_model), \n",
    "    ('rf', rf_model_best), \n",
    "    ('pr', pr_model)\n",
    "])\n",
    "er.fit(X_train_fe, y_train.values.ravel())\n",
    "y_pred_train = er.predict(X_train_fe)\n",
    "y_pred = er.predict(X_test_fe)\n",
    "\n",
    "'rmslr (train data): %.4f; rmslr (test data): %.4f' % (rmslr(y_train, y_pred_train), rmslr(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping of Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html\n",
    "# https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_early_stopping.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ensable Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kaggle = pd.read_csv('./data/test.csv', parse_dates=True)\n",
    "X_kaggle = df_kaggle[df.columns.difference(['count', 'registered', 'casual'])]\n",
    "y_kaggle = df['count']\n",
    "\n",
    "X_kaggle_fe = X_kaggle.sort_index().reset_index().drop(columns=['index'])\n",
    "y_kaggle = y_kaggle.sort_index().reset_index().drop(columns=['index'])\n",
    "\n",
    "X_kaggle_fe = extract_datetime_data(X_kaggle_fe)\n",
    "X_kaggle_fe = apply_min_max_scaller(X_kaggle_fe)\n",
    "X_kaggle_fe = pd.DataFrame(pt.fit_transform(X_kaggle_fe), columns=pt.get_feature_names_out())\n",
    "X_kaggle_fe.drop(columns=insignificant_features, inplace=True)\n",
    "\n",
    "df_kaggle = pd.DataFrame({\n",
    "    \"datetime\": X_kaggle['datetime'],\n",
    "    \"count\": np.ceil(rf_model_best.predict(X_kaggle_fe)).astype(int)\n",
    "})\n",
    "df_kaggle.to_csv(\"./output/kaggle_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission score **0.69915**  \n",
    "position 2498 out of 3242 submissions :("
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a36b59b37c4637ddb9c5034a0d11c20b9896a7105f0fa82c25b07f146a9279b2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
